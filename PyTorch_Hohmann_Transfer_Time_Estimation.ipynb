{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNiW8fuRFGcpBMMDAkpiiUP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/m-adeleke1/PyTorch_Projects/blob/main/PyTorch_Hohmann_Transfer_Time_Estimation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#What are we doing?\n",
        "\n",
        "We‚Äôre teaching a small neural network (a simple ‚Äúmath-guessing machine‚Äù) to estimate how long it takes a spacecraft to travel from one circular orbit around the Sun to another using an idealized maneuver called a Hohmann transfer. The Hohmann transfer is a classic, fuel-efficient way to move between two circular, coplanar orbits. The travel path is half of an ellipse whose ends just touch the two circular orbits. There‚Äôs a clean physics formula for the time this takes, so we actually know the right answer for any pair of radii. That makes this problem perfect for practice: we can generate a ton of training examples automatically.\n",
        "\n",
        "Think of it like this: you feed the network two numbers‚Äîr1 and r2 (the orbital radii, measured in Astronomical Units, AU)‚Äîand the network spits out one number‚Äît (days of flight time). During training, it tries many guesses, sees how wrong it was compared to the real physics formula, nudges its internal dials slightly, and repeats. After lots of practice, it learns an approximation of the formula.<br><br>\n",
        "\n",
        "##Why is this a great beginner project?\n",
        "\n",
        "Single input/output idea: only two inputs and one output. No scary high-dimensional data.\n",
        "\n",
        "Label generation is free: we don‚Äôt need a human to write answers; physics gives them to us.\n",
        "\n",
        "Clear success metrics: we can measure how close the model is to the real formula using standard numbers like MSE and RMSE (that‚Äôs just average squared error and its square root).\n",
        "\n",
        "Space theme: more fun than predicting house prices or iris petal lengths.<br><br>\n",
        "\n",
        "##What are the steps?\n",
        "\n",
        "Set up: import PyTorch and some helper libraries; fix random seeds for repeatability; pick CPU or GPU.\n",
        "\n",
        "Physics helpers + data: write a function that computes the true transfer time from r1 and r2. Then build a dataset by sampling many radii pairs and applying the formula. (Optionally add a tiny bit of noise to make the model more robust.)\n",
        "\n",
        "Split & normalize: split the data into train, validation, and test. Normalize inputs (and often outputs) so training is stable and fast.\n",
        "\n",
        "Dataset/DataLoader: wrap the arrays in a PyTorch Dataset and DataLoader so we can train in mini-batches.\n",
        "\n",
        "Model (MLP): define a tiny feed-forward neural network (a couple of Linear layers with ReLU in between) that maps the 2D input to a 1D output (the time).\n",
        "\n",
        "Train: run the training loop: forward pass ‚Üí loss ‚Üí backward pass (gradients) ‚Üí optimizer step. Use early stopping so we don‚Äôt overfit. Save the best weights.\n",
        "\n",
        "Evaluate & plot: compute RMSE on the test set, and show a scatter plot of predicted vs true days‚Äîgood fits hug the diagonal.\n",
        "\n",
        "Inference helpers: write utility functions that let you ask ‚ÄúEarth ‚Üí Mars?‚Äù easily or provide custom AU values.\n",
        "\n",
        "Save/Load: save the normalization constants and model weights so you can reload later.\n",
        "\n",
        "(Optional) Gradio app: a tiny web UI where you pick planets or slide custom AU values and see model vs physics side-by-side.<br><br>\n",
        "\n",
        "##What will you learn?\n",
        "\n",
        "How to turn a real-world concept into a supervised learning dataset.\n",
        "\n",
        "How to structure projects with train/val/test splits and normalization.\n",
        "\n",
        "How PyTorch batches data, defines models, computes losses, updates parameters, and saves weights.\n",
        "\n",
        "How to check a model‚Äôs quality with a plot and a simple RMSE number.\n",
        "\n",
        "How to add a mini UI for interactive experiments."
      ],
      "metadata": {
        "id": "QHIfzpo2QFcx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What you see in code: a ‚Äútitle‚Äù comment for Colab, then imports for math, NumPy, PyTorch, DataLoader stuff, Matplotlib, seeding, and device selection.\n",
        "\n",
        "What it does and why it‚Äôs here:\n",
        "\n",
        "###@title ... is just a Colab nicety‚Äîit lets you title the cell with a nice emoji and label. No effect on code.\n",
        "\n",
        "Python batteries:\n",
        "\n",
        "math, random, os ‚Äî built-in tools. math gives precise constants and functions; random for basic randomness; os for small filesystem tasks.\n",
        "\n",
        "numpy as np ‚Äî NumPy arrays are fast and friendly for numerical data (we‚Äôll hold our raw datasets here before converting to PyTorch tensors).\n",
        "\n",
        "PyTorch core:\n",
        "\n",
        "import torch brings in the framework.\n",
        "\n",
        "from torch import nn gives access to layers like nn.Linear, losses like nn.MSELoss, and utilities like nn.ReLU.\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader ‚Äî the standard way to wrap your arrays so the training loop can load mini-batches automatically (shuffling, batching, etc.).\n",
        "\n",
        "Matplotlib: import matplotlib.pyplot as plt ‚Äî to plot predicted vs true days later; visuals help you sanity-check performance.<br><br>\n",
        "\n",
        "##Seeding:\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
        "\n",
        "\n",
        "This sets the random seed for Python‚Äôs random, NumPy, and PyTorch so the experiment is repeatable. If you (or a friend) run the notebook again, you should get very similar results. Machine learning often has randomness (weight init, shuffling). Seeding reduces surprises.<br><br>\n",
        "\n",
        "\n",
        "##Device selection:\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device\n",
        "\n",
        "\n",
        "PyTorch can run on your CPU or your GPU (‚Äúcuda‚Äù). GPUs are faster for big models, but this toy is tiny, so either is fine. Still, it‚Äôs good practice to write device-aware code. Later, when we create tensors or move batches into the model, we‚Äôll do .to(device) so it all runs in the same place.<br><br>\n",
        "\n",
        "\n",
        "##Why this block matters:<br>\n",
        "\n",
        "It‚Äôs the skeleton for the rest of the notebook. You‚Äôve told Python which tools you need, fixed randomness, and decided where computations live. Even for small projects, getting these basics right makes everything smoother.<br><br>\n",
        "\n",
        "\n",
        "##Common pitfalls it avoids:\n",
        "\n",
        "Forgetting to seed (harder to debug variability).\n",
        "\n",
        "Mixing CPU and GPU tensors (causes runtime errors).\n",
        "\n",
        "Not importing Dataset/DataLoader (you end up writing clunky batching code)."
      ],
      "metadata": {
        "id": "RhSwUCDORGRl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BRZr7XjdpTpP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79538d36-79db-4a2e-ea07-8b33f0d1c184"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "#@title üöÄ Hohmann Transfer Time Learner (PyTorch + Colab)\n",
        "\n",
        "# This notebook trains a tiny MLP to approximate the physics formula for\n",
        "# Hohmann transfer time between two circular orbits around the Sun.\n",
        "\n",
        "import math, random, os\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##What you see in code:\n",
        "\n",
        "Constants:<br><br> AU_M (meters per AU)<br>\n",
        " MU_SUN (Sun‚Äôs gravitational parameter)<br> DAY_S (seconds per day)<br> and a dictionary of planet orbital radii in AU (PLANET_R_AU).\n",
        "\n",
        "hohmann_transfer_time_days(r1_au, r2_au): computes the true time (days) using the physics formula.\n",
        "\n",
        "Sampling helpers to create lots of input pairs.\n",
        "\n",
        "make_dataset(n, noise_frac): builds a large array of inputs and outputs.\n",
        "\n",
        "What it does and why it‚Äôs here:\n",
        "\n",
        "Units and constants:\n",
        "Physics math is picky about units. Our inputs (r1_au, r2_au) are in AU (Astronomical Units). The formula, however, needs meters and uses the Sun‚Äôs Œº (mu: GM) in SI units. So we convert AU ‚Üí meters (AU_M) and use MU_SUN (m¬≥/s¬≤). Finally, we convert seconds ‚Üí days with DAY_S. This strict unit handling keeps the numbers physically meaningful.\n",
        "\n",
        "The Hohmann transfer time formula:\n",
        "The half-ellipse transfer time is:\n",
        "\n",
        "t = œÄ * sqrt(a^3 / Œº)\n",
        "\n",
        "\n",
        "where a is the semi-major axis of the transfer ellipse, and for Hohmann between two circular orbits:\n",
        "a = (r1 + r2) / 2 (in meters).\n",
        "Œº is the gravitational parameter of the central body (the Sun here).<br><br>\n",
        "The function:\n",
        "\n",
        "Converts r1_au, r2_au to meters.\n",
        "\n",
        "Computes a = (r1_m + r2_m)/2.\n",
        "\n",
        "Evaluates the formula to get seconds.\n",
        "\n",
        "Returns seconds / DAY_S (days).\n",
        "\n",
        "This is our gold standard‚Äîthe label the neural net tries to learn.\n",
        "\n",
        "Planet radii (AU):\n",
        "The dict PLANET_R_AU provides typical orbit radii like Earth = 1.0 AU, Mars ‚âà 1.524 AU, etc. We‚Äôll sometimes sample training points exactly at these radii to make the model familiar with common planet pairs, not just random values.\n",
        "\n",
        "Sampling pairs:\n",
        "We make two kinds of examples:\n",
        "\n",
        "Random pairs: pick r1, r2 uniformly in a sensible range (e.g., 0.35‚Äì6 AU). This gives broad coverage.\n",
        "\n",
        "Planet pairs (about 30%): pick radii from the planet dict to make the dataset include familiar scenarios like Earth‚ÜîMars, Venus‚ÜîJupiter, etc.\n",
        "\n",
        "make_dataset:\n",
        "For n samples, we:\n",
        "\n",
        "Decide whether to create a planet pair (30% chance) or a random pair (70%).\n",
        "\n",
        "Compute the exact transfer time using the physics function.\n",
        "\n",
        "Optionally add tiny multiplicative noise (noise_frac) to simulate measurement or modeling imperfections. It‚Äôs small (e.g., 2%) and helps the model generalize rather than memorize.\n",
        "\n",
        "Append inputs [r1, r2] and label [t_days] to arrays.\n",
        "\n",
        "Return X (shape [n, 2], float32) and y (shape [n, 1], float32).\n",
        "\n",
        "Sanity printouts:\n",
        "A quick test prints a few input pairs and outputs so you can eyeball that numbers look reasonable (no NaNs, no crazy zeros).\n",
        "\n",
        "Why this block matters:\n",
        "It creates the entire learning problem out of thin air. No data hunting, no labeling budget‚Äîjust physics. For beginners, this is gold: you practice the entire PyTorch pipeline while having a trustworthy ‚Äúteacher‚Äù (the formula).\n",
        "\n",
        "Common pitfalls it avoids:\n",
        "\n",
        "Mixing units (AU vs meters) and getting nonsense answers.\n",
        "\n",
        "Not covering enough of the input space (the mix of random + planet pairs keeps variety).\n",
        "\n",
        "Targets too big/small (days are human-scale; we also normalize later)."
      ],
      "metadata": {
        "id": "lmDUjoKsSEvv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title üåû Physics constants + data generator\n",
        "\n",
        "AU_M = 1.495978707e11          # meters in 1 Astronomical Unit\n",
        "MU_SUN = 1.32712440018e20      # m^3/s^2 (standard gravitational parameter of the Sun)\n",
        "DAY_S = 86400.0                # seconds in a day\n",
        "\n",
        "# Planet mean orbital radii (AU). Feel free to add more.\n",
        "PLANET_R_AU = {\n",
        "    \"Mercury\": 0.387,\n",
        "    \"Venus\":   0.723,\n",
        "    \"Earth\":   1.000,\n",
        "    \"Mars\":    1.524,\n",
        "    \"Jupiter\": 5.204,\n",
        "}\n",
        "\n",
        "def hohmann_transfer_time_days(r1_au, r2_au):\n",
        "    \"\"\"\n",
        "    Ideal Hohmann transfer half-period (time of flight) in days\n",
        "    between two circular, coplanar orbits of radii r1 and r2 (in AU).\n",
        "    t = pi * sqrt(a^3 / mu), where a = (r1 + r2)/2 in meters.\n",
        "    \"\"\"\n",
        "    r1_m = r1_au * AU_M\n",
        "    r2_m = r2_au * AU_M\n",
        "    a = 0.5 * (r1_m + r2_m)\n",
        "    t_sec = math.pi * math.sqrt(a**3 / MU_SUN)\n",
        "    return t_sec / DAY_S\n",
        "\n",
        "# Quick check (Earth->Mars)\n",
        "print(\"Ideal Earth‚ÜíMars transfer time (days):\", round(hohmann_transfer_time_days(1.0, 1.524), 1))\n",
        "\n",
        "def sample_pair(min_au=0.35, max_au=6.0):\n",
        "    r1 = np.random.uniform(min_au, max_au)\n",
        "    r2 = np.random.uniform(min_au, max_au)\n",
        "    return r1, r2\n",
        "\n",
        "def make_dataset(n=5000, noise_frac=0.0):\n",
        "    \"\"\"\n",
        "    Create synthetic samples (r1,r2)-> time_days. Optionally add small noise\n",
        "    to the target to simulate model mismatch.\n",
        "    \"\"\"\n",
        "    xs = []\n",
        "    ys = []\n",
        "    for _ in range(n):\n",
        "        # 70% random pairs, 30% planet-to-planet pairs (for fun)\n",
        "        if np.random.rand() < 0.3:\n",
        "            r1 = random.choice(list(PLANET_R_AU.values()))\n",
        "            r2 = random.choice(list(PLANET_R_AU.values()))\n",
        "        else:\n",
        "            r1, r2 = sample_pair()\n",
        "        t = hohmann_transfer_time_days(r1, r2)\n",
        "        if noise_frac > 0:\n",
        "            t *= np.random.normal(1.0, noise_frac)\n",
        "        xs.append([r1, r2])\n",
        "        ys.append([t])\n",
        "    X = np.array(xs, dtype=np.float32)\n",
        "    y = np.array(ys, dtype=np.float32)\n",
        "    return X, y\n",
        "\n",
        "X, y = make_dataset(n=10)\n",
        "print(\"Example X (AU):\\n\", X[:3])\n",
        "print(\"Example y (days):\\n\", y[:3].flatten())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ltguo6KzN0gg",
        "outputId": "da1b3b05-2c9c-4ec7-efbc-f1c112beea71"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ideal Earth‚ÜíMars transfer time (days): 258.9\n",
            "Example X (AU):\n",
            " [[5.7215357 4.485766 ]\n",
            " [1.2315053 1.231369 ]\n",
            " [0.387     0.387    ]]\n",
            "Example y (days):\n",
            " [2105.6677    249.5668     43.967815]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What you see in code:\n",
        "\n",
        "A helper to split arrays into train, validation, and test sets by index shuffling.\n",
        "\n",
        "Generation of a larger dataset (e.g., 6,000 points).\n",
        "\n",
        "Normalization of inputs (and outputs) using the training set statistics only.\n",
        "\n",
        "What it does and why it‚Äôs here:\n",
        "\n",
        "Splitting the dataset:\n",
        "We shuffle indices and slice out three disjoint subsets:\n",
        "\n",
        "Train: what the model learns on.\n",
        "\n",
        "Validation: a small sample checked after each epoch to see if the model is getting better on unseen data (controls overfitting and helps choose when to stop).\n",
        "\n",
        "Test: a final, untouched sample used once at the end for the real-deal score.\n",
        "\n",
        "###Why shuffle?<br>\n",
        "So the three sets represent the same distribution of pairs. Why not peek at the test during training? Because it leads to ‚Äúcheating‚Äù‚Äîyou‚Äôll indirectly tune to that set and get an unrealistically good final score.<br>\n",
        "\n",
        "###Why normalize?<br>\n",
        "Neural nets learn faster and more stably when inputs (and often outputs) are roughly centered around 0 and have similar scale. If one feature is ~0.5 AU and another is ~5 AU, gradients can be unbalanced.<br><br>\n",
        "We compute:\n",
        "\n",
        "x_mean = mean(X_train, axis=0)<br>\n",
        "x_std  = std(X_train, axis=0) + 1e-8<br>\n",
        "X_norm = (X - x_mean) / x_std<br><br>\n",
        "\n",
        "\n",
        "We apply the same train-set mean/std to val and test. (Never compute stats on validation or test‚Äîthey must simulate ‚Äúfuture‚Äù data the model hasn‚Äôt seen.)<br><br>\n",
        "\n",
        "###Output normalization (optional but helpful):<br>\n",
        "Targets (days) can have a wide range. Normalizing them to mean 0 and std 1 often improves regression training. We compute y_mean and y_std on the training targets and normalize y_tr, y_val, y_te with those. Later we‚Äôll de-normalize predictions to report RMSE in actual days, which is easier to understand.\n",
        "\n",
        "Shapes:\n",
        "\n",
        "X_tr_n has shape [N_train, 2].\n",
        "\n",
        "y_tr_n has shape [N_train, 1].\n",
        "Both are NumPy arrays for now; we‚Äôll convert to tensors in the Dataset step.<br><br>\n",
        "\n",
        "###Why a larger dataset here (e.g., 6,000)?<br>\n",
        "The function earlier did a tiny sample just to show the format. Now we make a bigger set to train a more reliable model. This number is still tiny by ML standards, but more than enough since this function is smooth and low-dimensional.<br>\n",
        "\n",
        "###Why this block matters:\n",
        "It formalizes good ML hygiene: proper splits and proper normalization. These two habits transfer to almost every project you‚Äôll ever do.<br>\n",
        "\n",
        "###Common pitfalls it avoids:\n",
        "\n",
        "Leakage: computing normalization stats on the full dataset (includes val/test ‚Üí overly optimistic results).<br>\n",
        "\n",
        "###Not normalizing at all (training can still work, but often slower/less stable).\n",
        "\n",
        "Accidentally shuffling inputs and labels independently (always shuffle indices, then slice X and y with the same index order)."
      ],
      "metadata": {
        "id": "gbxOg297TMl6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title üì¶ Split & normalize\n",
        "\n",
        "def train_val_test_split(X, y, val_ratio=0.15, test_ratio=0.15):\n",
        "    n = len(X)\n",
        "    idx = np.arange(n)\n",
        "    np.random.shuffle(idx)\n",
        "    n_test = int(n * test_ratio)\n",
        "    n_val  = int(n * val_ratio)\n",
        "    test_idx = idx[:n_test]\n",
        "    val_idx  = idx[n_test:n_test+n_val]\n",
        "    train_idx = idx[n_test+n_val:]\n",
        "    return (X[train_idx], y[train_idx],\n",
        "            X[val_idx],  y[val_idx],\n",
        "            X[test_idx], y[test_idx])\n",
        "\n",
        "# Generate a bigger dataset\n",
        "X, y = make_dataset(n=6000, noise_frac=0.02)\n",
        "\n",
        "X_tr, y_tr, X_val, y_val, X_te, y_te = train_val_test_split(X, y, 0.15, 0.15)\n",
        "\n",
        "# Normalize inputs for stabler training\n",
        "x_mean = X_tr.mean(axis=0, keepdims=True)\n",
        "x_std  = X_tr.std(axis=0, keepdims=True) + 1e-8\n",
        "X_tr_n = (X_tr - x_mean) / x_std\n",
        "X_val_n= (X_val - x_mean) / x_std\n",
        "X_te_n = (X_te - x_mean) / x_std\n",
        "\n",
        "# Optionally normalize outputs (often helps for regression)\n",
        "y_mean = y_tr.mean(axis=0, keepdims=True)\n",
        "y_std  = y_tr.std(axis=0, keepdims=True) + 1e-8\n",
        "y_tr_n = (y_tr - y_mean) / y_std\n",
        "y_val_n= (y_val - y_mean) / y_std\n",
        "y_te_n = (y_te - y_mean) / y_std\n",
        "\n",
        "print(\"Train size:\", len(X_tr), \"Val:\", len(X_val), \"Test:\", len(X_te))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7MSc3-q5N4Jr",
        "outputId": "06925de5-4c5e-4f84-a7be-28b400ce6e39"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size: 4200 Val: 900 Test: 900\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What you see in code:\n",
        "\n",
        "A tiny class PairsDataset(Dataset) that wraps (X, y) NumPy arrays and returns PyTorch tensors one row at a time.\n",
        "\n",
        "Construction of train_ds, val_ds, test_ds.\n",
        "\n",
        "DataLoaders for each split with a batch size and shuffle options.\n",
        "\n",
        "A quick next(iter(train_loader)) to prove it yields batches.<br>\n",
        "\n",
        "###What it does and why it‚Äôs here:\n",
        "\n",
        "Dataset class:<br>\n",
        "PyTorch training loops expect data that can be accessed like dataset[i] and knows its length (len(dataset)). <br>\n",
        "\n",
        "Our PairsDataset:<br>\n",
        "Converts the NumPy arrays into torch tensors one time in __init__ (so we don‚Äôt convert every batch).\n",
        "\n",
        "Implements __len__ (returns number of rows).\n",
        "\n",
        "Implements __getitem__(idx) to return a pair (X[idx], y[idx]). Each element is a row tensor:\n",
        "\n",
        "X[idx] shape: [2] (two inputs)\n",
        "\n",
        "y[idx] shape: [1] (one target)\n",
        "\n",
        "This design is super common and easy to modify for other projects (images, text, etc.).\n",
        "\n",
        "DataLoader:\n",
        "The DataLoader wraps a Dataset and handles batching and shuffling:\n",
        "\n",
        "batch_size=128 means each training step uses 128 examples at once. Batched operations run faster on hardware and make gradient estimates stable.\n",
        "\n",
        "shuffle=True for the training loader means every epoch will randomize the order of rows. That helps the optimizer not get stuck by seeing the same pattern order each time.\n",
        "\n",
        "Validation/test loaders use shuffle=False because order doesn‚Äôt matter there; we‚Äôre just aggregating metrics.\n",
        "\n",
        "drop_last=False means we keep the final smaller batch if the dataset size isn‚Äôt divisible by 128.\n",
        "\n",
        "When you loop over train_loader, each xb (x-batch) is shape [B, 2] and each yb (y-batch) is shape [B, 1], where B is the batch size (often 128, but the final batch might be smaller).\n",
        "\n",
        "Why batch at all?\n",
        "Neural nets learn via gradient descent, which uses the slope of the loss wrt weights. Instead of computing that slope using the entire dataset (slow), we approximate it using a small mini-batch (fast). This ‚Äústochastic‚Äù (randomized) process turns out to work incredibly well in practice.\n",
        "\n",
        "Quick smoke test:\n",
        "next(iter(train_loader)) asks the loader for the first batch and prints the shapes. This catches silly mistakes early:\n",
        "\n",
        "swapped dimensions,\n",
        "\n",
        "y not the right shape,\n",
        "\n",
        "dataset length zero because you filtered everything away, etc.\n",
        "\n",
        "Why this block matters:\n",
        "It gives you the standard PyTorch data pipeline you‚Äôll reuse endlessly: Dataset ‚Üí DataLoader ‚Üí batches in the training loop. Getting comfortable with this pattern is a big step toward being productive in PyTorch.\n",
        "\n",
        "Common pitfalls it avoids:\n",
        "\n",
        "Returning Python lists or NumPy arrays directly from __getitem__ (PyTorch will still try to collate them, but it‚Äôs cleaner and safer to return tensors).\n",
        "\n",
        "Forgetting to set shuffle=True for training (can slow learning or cause weird plateaus).\n",
        "\n",
        "Mixing normalized and unnormalized arrays (here all splits are already normalized consistently)."
      ],
      "metadata": {
        "id": "ZFS6_l2hUzWm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title üß∞ PyTorch Dataset & DataLoader\n",
        "class PairsDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.from_numpy(X)\n",
        "        self.y = torch.from_numpy(y)\n",
        "    def __len__(self): return len(self.X)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "batch_size = 128\n",
        "train_ds = PairsDataset(X_tr_n, y_tr_n)\n",
        "val_ds   = PairsDataset(X_val_n, y_val_n)\n",
        "test_ds  = PairsDataset(X_te_n, y_te_n)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=False)\n",
        "val_loader   = DataLoader(val_ds, batch_size=batch_size, shuffle=False, drop_last=False)\n",
        "test_loader  = DataLoader(test_ds, batch_size=batch_size, shuffle=False, drop_last=False)\n",
        "\n",
        "next(iter(train_loader))[0].shape, next(iter(train_loader))[1].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V30CCjgEOFnZ",
        "outputId": "2afedc22-b874-490d-835a-a5d2fa145e47"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([128, 2]), torch.Size([128, 1]))"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model: a tiny MLP regressor (2‚Üí64‚Üí64‚Üí1)<br><br>\n",
        "\n",
        "What it is:<br><br>\n",
        "A multilayer perceptron (MLP) is the simplest useful neural network: a stack of fully-connected layers with nonlinear activations (here, ReLU). Our inputs are two numbers‚Äîr1 and r2 (normalized)‚Äîand our output is one number: predicted transfer time (normalized). The model learns a squishy function f([r1, r2]) ‚Üí days.\n",
        "\n",
        "Code anatomy:\n",
        "\n",
        "nn.Linear(in_dim, hid) is a dense layer: it multiplies the input vector by a weight matrix and adds a bias. If x is [B, 2] (B = batch size), the output is [B, 64]. Think of each of the 64 ‚Äúneurons‚Äù as learning a different pattern in the radii inputs.\n",
        "\n",
        "nn.ReLU() turns negative values into zeros (max(0, x)). Without nonlinearities, stacking linear layers is still just‚Ä¶ linear. ReLU gives the network bendiness to fit curves.\n",
        "\n",
        "We stack another Linear(64, 64) + ReLU() to increase capacity (more bendy). Two tiny hidden layers are plenty for a smooth 2D‚Üí1D mapping.\n",
        "\n",
        "nn.Dropout(p) randomly zeroes a fraction p of activations during training. This is a regularizer that prevents the network from relying too heavily on any single neuron. Here p=0.0 by default because the task is simple; you can set p=0.1‚Äì0.3 to test robustness.\n",
        "\n",
        "Final nn.Linear(64, 1) converts the 64-dim ‚Äúsummary‚Äù to a single number. No activation here because we‚Äôre predicting a continuous value that could be anywhere on the real line (remember, we normalized y).\n",
        "\n",
        "Why this structure works:\n",
        "\n",
        "With just two inputs, the function you‚Äôre approximating (Hohmann time) is smooth and nicely shaped. A small MLP can learn it fast.\n",
        "\n",
        "ReLU layers make it a universal approximator (fancy way to say: with enough hidden units, it can get arbitrarily close to the true function).\n",
        "\n",
        "It‚Äôs easy to reason about shapes:\n",
        "\n",
        "input: [B, 2]\n",
        "\n",
        "hidden: [B, 64] ‚Üí [B, 64]\n",
        "\n",
        "output: [B, 1]\n",
        "\n",
        "Parameter count intuition:\n",
        "\n",
        "First layer: 2 * 64 + 64 (weights + biases) = 192\n",
        "\n",
        "Second layer: 64 * 64 + 64 = 4160\n",
        "\n",
        "Final layer: 64 * 1 + 1 = 65\n",
        "Total ‚âà 4417 parameters‚Äîtiny! That‚Äôs good: small model, low risk of overfitting, quick to train.\n",
        "\n",
        "Common pitfalls avoided:\n",
        "\n",
        "No activation on the output for regression (correct).\n",
        "\n",
        "Matching the normalized output: the model predicts y_n, not raw days; we denormalize later.\n",
        "\n",
        "Using nn.Sequential keeps the forward clean and reduces wiring mistakes."
      ],
      "metadata": {
        "id": "etFjd89NVi5i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title üß† MLP regressor (2‚Üí64‚Üí64‚Üí1)\n",
        "\n",
        "class MLPRegressor(nn.Module):\n",
        "    def __init__(self, in_dim=2, hid=64, out_dim=1, p=0.0):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim, hid),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hid, hid),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p),\n",
        "            nn.Linear(hid, out_dim),\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "model = MLPRegressor().to(device)\n",
        "sum(p.numel() for p in model.parameters())/1e3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZHx6tuz5OLaY",
        "outputId": "d231cc75-5497-4b37-cbe9-59e1354b8b3f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.417"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What it is:<br>\n",
        "The classic PyTorch rhythm: forward ‚Üí loss ‚Üí backward ‚Üí step, repeated for mini-batches and epochs. We also track a validation score to pick the best model and avoid overfitting.\n",
        "\n",
        "Key pieces:\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-3, weight_decay=1e-4)\n",
        "\n",
        "\n",
        "MSELoss (mean squared error) is standard for regression. It punishes big mistakes heavily (squared).\n",
        "\n",
        "AdamW is a modern optimizer that usually just‚Ä¶ works. lr=3e-3 is a safe starting point for small MLPs. weight_decay adds L2 regularization (another guard against overfitting).\n",
        "\n",
        "def rmse_denorm(y_pred_n, y_true_n, y_mean, y_std):\n",
        "    y_pred = y_pred_n * y_std + y_mean\n",
        "    y_true = y_true_n * y_std + y_mean\n",
        "    return torch.sqrt(torch.mean((y_pred - y_true)**2))\n",
        "\n",
        "\n",
        "During training we optimize on the normalized target, but to report an understandable number we compute RMSE in days by de-normalizing predictions and truths. RMSE is ‚Äútypical error size‚Äù in the original units (days), which is more meaningful than ‚Äúloss = 0.003‚Äù.\n",
        "\n",
        "Epoch loop flow:\n",
        "\n",
        "Train mode: model.train() enables dropout (if any) and turns on gradient tracking.\n",
        "\n",
        "For each batch from train_loader:\n",
        "\n",
        "Move xb, yb to device.\n",
        "\n",
        "pred = model(xb) ‚Üí forward pass.\n",
        "\n",
        "loss = criterion(pred, yb) ‚Üí a single number.\n",
        "\n",
        "optimizer.zero_grad() resets old gradients.\n",
        "\n",
        "loss.backward() computes gradients via autograd.\n",
        "\n",
        "nn.utils.clip_grad_norm_(...) trims very large gradients (stability).\n",
        "\n",
        "optimizer.step() nudges parameters opposite the gradient (learning).\n",
        "\n",
        "Validation: model.eval() and torch.no_grad() (saves memory; faster). Loop over val_loader, compute loss and RMSE (in days).\n",
        "\n",
        "Early stopping: if val_loss improves, save weights to best_hohmann.pt. If it doesn‚Äôt improve for patience epochs, stop.\n",
        "\n",
        "Why these choices:\n",
        "\n",
        "Mini-batches (e.g., 128) give stable gradient estimates and hardware efficiency.\n",
        "\n",
        "Gradient clipping prevents rare exploding gradients from wrecking learning.\n",
        "\n",
        "Early stopping protects against overfitting and saves your best model automatically.\n",
        "\n",
        "Reading the logs:\n",
        "Every ~20 epochs we print train MSE, val MSE, and val RMSE (days). Ideally: train down, val down, then both flatten. If train keeps dropping but val rises, you‚Äôre overfitting: reduce capacity, increase regularization, add data, or stop earlier.\n",
        "\n",
        "Common pitfalls avoided:\n",
        "\n",
        "Forgetting model.eval() during validation (dropout would stay on).\n",
        "\n",
        "Forgetting zero_grad() (gradients accumulate by default in PyTorch).\n",
        "\n",
        "Not saving the best model (you might end with worse weights at the final epoch)."
      ],
      "metadata": {
        "id": "Vjv43lcAWPZC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title üöÇ Train\n",
        "def rmse_denorm(y_pred_n, y_true_n, y_mean, y_std):\n",
        "    y_pred = y_pred_n * y_std + y_mean\n",
        "    y_true = y_true_n * y_std + y_mean\n",
        "    return torch.sqrt(torch.mean((y_pred - y_true)**2))\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-3, weight_decay=1e-4)\n",
        "\n",
        "best_val = float(\"inf\")\n",
        "patience, waited = 10, 0\n",
        "EPOCHS = 200\n",
        "\n",
        "train_hist, val_hist = [], []\n",
        "\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    # train\n",
        "    model.train()\n",
        "    running = 0.0\n",
        "    for xb, yb in train_loader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        pred = model(xb)\n",
        "        loss = criterion(pred, yb)\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        running += loss.item() * xb.size(0)\n",
        "    train_loss = running / len(train_ds)\n",
        "    train_hist.append(train_loss)\n",
        "\n",
        "    # validate\n",
        "    model.eval()\n",
        "    val_running = 0.0\n",
        "    rmse_running = 0.0\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in val_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            pred = model(xb)\n",
        "            val_running += criterion(pred, yb).item() * xb.size(0)\n",
        "            rmse_running += rmse_denorm(pred, yb,\n",
        "                                        torch.from_numpy(y_mean).to(device),\n",
        "                                        torch.from_numpy(y_std).to(device)).item() * xb.size(0)\n",
        "    val_loss = val_running / len(val_ds)\n",
        "    val_rmse_days = rmse_running / len(val_ds)\n",
        "    val_hist.append(val_loss)\n",
        "\n",
        "    if val_loss < best_val - 1e-6:\n",
        "        best_val = val_loss\n",
        "        waited = 0\n",
        "        torch.save(model.state_dict(), \"best_hohmann.pt\")\n",
        "    else:\n",
        "        waited += 1\n",
        "        if waited >= patience:\n",
        "            print(f\"Early stop at epoch {epoch}\")\n",
        "            break\n",
        "\n",
        "    if epoch % 20 == 0 or epoch == 1:\n",
        "        print(f\"Epoch {epoch:3d} | train MSE={train_loss:.4f} | val MSE={val_loss:.4f} | val RMSE (days)={val_rmse_days:.2f}\")\n",
        "\n",
        "print(\"Best val MSE:\", best_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KMhcAH5AOOIA",
        "outputId": "6abe303d-4124-4bdd-9138-ccf6723e1ea6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch   1 | train MSE=0.1365 | val MSE=0.0104 | val RMSE (days)=60.90\n",
            "Early stop at epoch 17\n",
            "Best val MSE: 0.0014170538112779872\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What it is:<br>\n",
        "A proper final exam using data the model never saw during training or validation. We report one clean number (RMSE in days), then draw a predicted vs. true scatter plot.\n",
        "\n",
        "Code highlights:\n",
        "\n",
        "model.load_state_dict(torch.load(\"best_hohmann.pt\", map_location=device))\n",
        "model.eval()\n",
        "\n",
        "\n",
        "We reload the best weights found during early stopping and switch to eval mode.\n",
        "\n",
        "def predict_denorm(Xn):\n",
        "    with torch.no_grad():\n",
        "        Xn_t = torch.from_numpy(Xn).to(device)\n",
        "        yp_n = model(Xn_t).cpu().numpy()\n",
        "    return yp_n * y_std + y_mean\n",
        "\n",
        "\n",
        "This helper runs the model on normalized inputs and returns denormalized predictions (days). It‚Äôs useful in many places (test set, UI, quick checks).\n",
        "\n",
        "y_pred_te = predict_denorm(X_te_n)\n",
        "rmse_test = np.sqrt(np.mean((y_pred_te - y_te)**2))\n",
        "\n",
        "\n",
        "Compute RMSE directly in numpy. Lower is better. For this task, small single-digit RMSE (days) is achievable.\n",
        "\n",
        "Scatter plot:\n",
        "\n",
        "X-axis: True days; Y-axis: Predicted days.\n",
        "\n",
        "The dashed diagonal is perfect predictions.\n",
        "\n",
        "If points hug the diagonal with little spread, the model nailed the mapping.\n",
        "\n",
        "Systematic curve away from diagonal suggests bias (e.g., underpredicting long transfers). Consider capacity tweaks (more hidden units) or training longer.\n",
        "\n",
        "Why it matters:\n",
        "Numbers alone can hide shape mistakes. The plot lets you see whether errors are random or patterned:\n",
        "\n",
        "Random scatter around the diagonal: good generalization.\n",
        "\n",
        "Bowed curve: model learned a simpler function (e.g., linear-ish) and needs more flexibility.\n",
        "\n",
        "Two clouds: your data distribution might have modes (e.g., inner vs. outer orbits). You could train separate models or add features.\n",
        "\n",
        "Common pitfalls avoided:\n",
        "\n",
        "Evaluating on the validation set instead of the test set (gives too optimistic results).\n",
        "\n",
        "Forgetting to denormalize (you‚Äôd report RMSE in normalized units, which is not intuitive)."
      ],
      "metadata": {
        "id": "yLlF5v6kWtXH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title üìà Test evaluation (RMSE in days) + scatter plot\n",
        "model.load_state_dict(torch.load(\"best_hohmann.pt\", map_location=device))\n",
        "model.eval()\n",
        "\n",
        "def predict_denorm(Xn):\n",
        "    with torch.no_grad():\n",
        "        Xn_t = torch.from_numpy(Xn).to(device)\n",
        "        yp_n = model(Xn_t).cpu().numpy()\n",
        "    return yp_n * y_std + y_mean\n",
        "\n",
        "y_pred_te = predict_denorm(X_te_n)\n",
        "rmse_test = np.sqrt(np.mean((y_pred_te - y_te)**2))\n",
        "print(f\"Test RMSE: {rmse_test:.2f} days\")\n",
        "\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.scatter(y_te, y_pred_te, s=10, alpha=0.6)\n",
        "mn = float(min(y_te.min(), y_pred_te.min()))\n",
        "mx = float(max(y_te.max(), y_pred_te.max()))\n",
        "plt.plot([mn, mx], [mn, mx], linestyle=\"--\")\n",
        "plt.xlabel(\"True transfer time (days)\")\n",
        "plt.ylabel(\"Predicted transfer time (days)\")\n",
        "plt.title(\"Predicted vs True (Test)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        },
        "id": "M3FdTogUOddL",
        "outputId": "b980acc5-9dea-4ea7-990c-a2d6fd539ec9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test RMSE: 22.38 days\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What it is:\n",
        "Friendly functions so you can query the model in natural ways: planets by name or custom radii in AU. They also print the physics baseline so you can compare model vs. truth instantly.\n",
        "\n",
        "Key functions:\n",
        "\n",
        "def normalize_inputs(X):\n",
        "    return (X - x_mean) / x_std\n",
        "\n",
        "\n",
        "Applies the training mean/std (saved from earlier) to new inputs. This is crucial: the model expects normalized features because that‚Äôs how it learned.\n",
        "\n",
        "def predict_days(r1_au, r2_au):\n",
        "    Xn = normalize_inputs(np.array([[r1_au, r2_au]], dtype=np.float32))\n",
        "    yp = predict_denorm(Xn)[0,0]\n",
        "    true = hohmann_transfer_time_days(r1_au, r2_au)\n",
        "    return float(yp), float(true)\n",
        "\n",
        "\n",
        "One-stop shop: normalize inputs, run the model, denormalize the output, and compute the exact physics number for comparison.\n",
        "\n",
        "def predict_planets(p1, p2):\n",
        "    r1 = PLANET_R_AU[p1]; r2 = PLANET_R_AU[p2]\n",
        "    yp, yt = predict_days(r1, r2)\n",
        "    print(f\"{p1} ({r1} AU) ‚Üí {p2} ({r2} AU)\")\n",
        "    print(f\" Model: {yp:.1f} days | Physics: {yt:.1f} days | Abs err: {abs(yp-yt):.1f} days\")\n",
        "\n",
        "\n",
        "Convenience for well-known targets. Try a few and see if errors are tiny‚Äîif not, it hints at where the model is weaker.\n",
        "\n",
        "Why it matters:\n",
        "Inference code is what you‚Äôll actually use in a script, a service, or a UI. It also demonstrates good hygiene:\n",
        "\n",
        "Always apply the same preprocessing at inference that you used during training.\n",
        "\n",
        "Provide reference values (here, the exact physics) when possible. It builds trust and makes debugging easy.\n",
        "\n",
        "Common pitfalls avoided:\n",
        "\n",
        "Using raw AU inputs directly with a model trained on normalized features (predictions will be nonsense).\n",
        "\n",
        "Not aligning units (mixing AU and meters).\n",
        "\n",
        "Swallowing exceptions on bad input (we include simple checks in the Gradio block)."
      ],
      "metadata": {
        "id": "byGWvwVnW-aJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title üõ∞Ô∏è Inference: planet‚Üíplanet or custom AU\n",
        "\n",
        "def normalize_inputs(X):\n",
        "    return (X - x_mean) / x_std\n",
        "\n",
        "def predict_days(r1_au, r2_au):\n",
        "    \"\"\"Model prediction (days) + physics baseline for comparison.\"\"\"\n",
        "    Xn = normalize_inputs(np.array([[r1_au, r2_au]], dtype=np.float32))\n",
        "    yp = predict_denorm(Xn)[0,0]\n",
        "    true = hohmann_transfer_time_days(r1_au, r2_au)\n",
        "    return float(yp), float(true)\n",
        "\n",
        "def predict_planets(p1, p2):\n",
        "    r1 = PLANET_R_AU[p1]\n",
        "    r2 = PLANET_R_AU[p2]\n",
        "    yp, yt = predict_days(r1, r2)\n",
        "    print(f\"{p1} ({r1} AU) ‚Üí {p2} ({r2} AU)\")\n",
        "    print(f\" Model: {yp:.1f} days | Physics: {yt:.1f} days | Abs err: {abs(yp-yt):.1f} days\")\n",
        "\n",
        "# Examples\n",
        "predict_planets(\"Earth\", \"Mars\")\n",
        "predict_planets(\"Venus\", \"Jupiter\")\n",
        "\n",
        "# Custom values\n",
        "yp, yt = predict_days(0.9, 1.7)\n",
        "print(f\"\\nCustom 0.9 AU ‚Üí 1.7 AU | Model: {yp:.1f} days | Physics: {yt:.1f} days\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mnmZ8Gu1OlfD",
        "outputId": "fa4dc474-e1a7-4807-c516-a7d38ccb07cb"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Earth (1.0 AU) ‚Üí Mars (1.524 AU)\n",
            " Model: 260.9 days | Physics: 258.9 days | Abs err: 2.0 days\n",
            "Venus (0.723 AU) ‚Üí Jupiter (5.204 AU)\n",
            " Model: 933.7 days | Physics: 931.7 days | Abs err: 2.0 days\n",
            "\n",
            "Custom 0.9 AU ‚Üí 1.7 AU | Model: 273.4 days | Physics: 270.7 days\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What it is:\n",
        "A tiny persistence layer so your trained model is reusable without retraining. You save:\n",
        "\n",
        "The model weights (best_hohmann.pt).\n",
        "\n",
        "The preprocessing constants (x_mean, x_std, y_mean, y_std) and the planet dictionary in preproc.json.\n",
        "\n",
        "Code highlights:\n",
        "\n",
        "artifacts = {\n",
        "    \"x_mean\": x_mean.tolist(),\n",
        "    \"x_std\":  x_std .tolist(),\n",
        "    \"y_mean\": y_mean.tolist(),\n",
        "    \"y_std\":  y_std .tolist(),\n",
        "    \"planet_r_au\": PLANET_R_AU,\n",
        "}\n",
        "with open(\"preproc.json\", \"w\") as f:\n",
        "    json.dump(artifacts, f)\n",
        "\n",
        "\n",
        "NumPy arrays aren‚Äôt directly JSON-serializable, so we use .tolist().\n",
        "\n",
        "Reload sanity check:\n",
        "\n",
        "with open(\"preproc.json\") as f:\n",
        "    art2 = json.load(f)\n",
        "assert \"x_mean\" in art2 and \"planet_r_au\" in art2\n",
        "\n",
        "\n",
        "Quick check that the file exists and has the keys we need.\n",
        "\n",
        "Why it matters:\n",
        "In the real world you deploy trained models, not training code. Saving preprocessing stats is critical: without the exact same normalization, the model‚Äôs outputs drift. Keeping them together avoids mismatches.\n",
        "\n",
        "Common pitfalls avoided:\n",
        "\n",
        "Only saving weights and forgetting the normalization constants.\n",
        "\n",
        "Changing the preprocessing later and reusing old weights (silent accuracy drop).\n",
        "\n",
        "Not versioning: consider naming files with timestamps or a semantic version if you iterate often."
      ],
      "metadata": {
        "id": "ox-ut-XXXG-8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title üíæ Save/load artifacts\n",
        "import json\n",
        "\n",
        "artifacts = {\n",
        "    \"x_mean\": x_mean.tolist(),\n",
        "    \"x_std\":  x_std .tolist(),\n",
        "    \"y_mean\": y_mean.tolist(),\n",
        "    \"y_std\":  y_std .tolist(),\n",
        "    \"planet_r_au\": PLANET_R_AU,\n",
        "}\n",
        "with open(\"preproc.json\", \"w\") as f:\n",
        "    json.dump(artifacts, f)\n",
        "\n",
        "# Reload demo\n",
        "with open(\"preproc.json\") as f:\n",
        "    art2 = json.load(f)\n",
        "assert \"x_mean\" in art2 and \"planet_r_au\" in art2\n",
        "print(\"Artifacts saved & sanity-checked.\")"
      ],
      "metadata": {
        "id": "Yjsd8JwDOqsz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What it is:\n",
        "A tiny web UI built with Gradio so anyone can play with the model in a browser: pick planets from dropdowns or type custom AU radii; the app prints Model vs Physics and the absolute error.\n",
        "\n",
        "Code anatomy:\n",
        "\n",
        "!pip -q install gradio==4.* > /dev/null\n",
        "import gradio as gr\n",
        "\n",
        "\n",
        "Installs and imports Gradio in Colab.\n",
        "\n",
        "def ui_predict(p1, p2, r1, r2, use_planets):\n",
        "    if use_planets:\n",
        "        r1 = PLANET_R_AU[p1]; r2 = PLANET_R_AU[p2]\n",
        "    else:\n",
        "        if r1 <= 0 or r2 <= 0:\n",
        "            return \"Radii must be > 0 AU.\"\n",
        "    yp, yt = predict_days(r1, r2)\n",
        "    return (f\"Inputs: r1={r1:.3f} AU, r2={r2:.3f} AU\\n\"\n",
        "            f\"Model:   {yp:.1f} days\\n\"\n",
        "            f\"Physics: {yt:.1f} days\\n\"\n",
        "            f\"Abs err: {abs(yp-yt):.1f} days\")\n",
        "\n",
        "\n",
        "Core function the UI calls. It supports two modes: (A) choose planets; (B) type custom radii. It validates inputs (no non-positive AU), calls your inference helper, and prints a neat, human-readable result.\n",
        "\n",
        "demo = gr.Interface(\n",
        "    fn=ui_predict,\n",
        "    inputs=[Dropdown, Dropdown, Slider, Slider, Checkbox],\n",
        "    outputs=\"text\",\n",
        "    title=\"Hohmann Transfer Time Learner\",\n",
        "    description=\"Choose planets or enter custom AU radii. Shows model vs physics time-of-flight.\",\n",
        ")\n",
        "demo.launch()\n",
        "\n",
        "\n",
        "A very small UI: two dropdowns for planets, two sliders for custom AU, and a toggle checkbox to choose which input method to use. Output is plain text (simple and robust).\n",
        "\n",
        "Why it matters:\n",
        "A hands-on panel makes the project feel alive. You can try Earth‚ÜíMars, Venus‚ÜíJupiter, or weird custom pairs in seconds. It also forces you to think about input validation and user experience‚Äîskills you need for real apps.\n",
        "\n",
        "Common pitfalls avoided:\n",
        "\n",
        "Launching the app without checking the model is loaded (we already loaded best weights earlier).\n",
        "\n",
        "Not guarding invalid inputs (like r1 = -1).\n",
        "\n",
        "Hiding differences between model and physics‚Äîshowing both builds intuition and trust."
      ],
      "metadata": {
        "id": "NgArKY8YXOxu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title üß™ Gradio mini-app (optional)\n",
        "!pip -q install gradio==4.* > /dev/null\n",
        "\n",
        "import gradio as gr\n",
        "\n",
        "def ui_predict(p1, p2, r1, r2, use_planets):\n",
        "    if use_planets:\n",
        "        try:\n",
        "            r1 = PLANET_R_AU[p1]\n",
        "            r2 = PLANET_R_AU[p2]\n",
        "        except KeyError:\n",
        "            return \"Unknown planet name.\"\n",
        "    else:\n",
        "        if r1 <= 0 or r2 <= 0:\n",
        "            return \"Radii must be > 0 AU.\"\n",
        "    yp, yt = predict_days(r1, r2)\n",
        "    return (f\"Inputs: r1={r1:.3f} AU, r2={r2:.3f} AU\\n\"\n",
        "            f\"Model:   {yp:.1f} days\\n\"\n",
        "            f\"Physics: {yt:.1f} days\\n\"\n",
        "            f\"Abs err: {abs(yp-yt):.1f} days\")\n",
        "\n",
        "demo = gr.Interface(\n",
        "    fn=ui_predict,\n",
        "    inputs=[\n",
        "        gr.Dropdown(list(PLANET_R_AU.keys()), value=\"Earth\", label=\"From planet\"),\n",
        "        gr.Dropdown(list(PLANET_R_AU.keys()), value=\"Mars\", label=\"To planet\"),\n",
        "        gr.Slider(0.3, 6.0, value=1.0, step=0.001, label=\"Custom r1 (AU)\"),\n",
        "        gr.Slider(0.3, 6.0, value=1.5, step=0.001, label=\"Custom r2 (AU)\"),\n",
        "        gr.Checkbox(True, label=\"Use planet dropdowns\"),\n",
        "    ],\n",
        "    outputs=\"text\",\n",
        "    title=\"Hohmann Transfer Time Learner\",\n",
        "    description=\"Choose planets or enter custom AU radii. Shows model vs physics time-of-flight.\",\n",
        ")\n",
        "demo.launch(debug=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 712
        },
        "id": "dK34MazgOv7m",
        "outputId": "a5edfc56-2645-4c0f-8dec-eddd91553c73"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dataproc-spark-connect 0.8.3 requires websockets>=14.0, but you have websockets 12.0 which is incompatible.\n",
            "yfinance 0.2.65 requires websockets>=13.0, but you have websockets 12.0 which is incompatible.\n",
            "google-adk 1.13.0 requires websockets<16.0.0,>=15.0.1, but you have websockets 12.0 which is incompatible.\n",
            "google-genai 1.33.0 requires websockets<15.1.0,>=13.0.0, but you have websockets 12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSetting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://4a788f1b035cb290dd.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://4a788f1b035cb290dd.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    }
  ]
}